{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6a22b5",
   "metadata": {},
   "source": [
    "# Assignment 1: Data Exploration and Classification\n",
    "\n",
    "<strong style=\"color: red;\">Due date: 23rd, January, 2025</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e015f6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment, you will implement a data preprocessing pipeline to explore textual data, and another to perform text classification. The assignment is designed in a way where each part and its component are independent, whenever possible. Hence, if you feel stuck in one section, you can move to the next.\n",
    "\n",
    "### Requirements\n",
    "In order execute this notebook you will need to have `python >= 3.9` installed on your machine. As for dependencies, you will need:  \n",
    "- [Jupyter Notebook](https://jupyter.org/install#jupyter-notebook), to run this notebook.\n",
    "- [scikit-learn](https://scikit-learn.org/stable/install.html), to train the classical ML models needed in Part 2 of the assignment.\n",
    "- [pandas](https://pandas.pydata.org/docs/getting_started/install.html)\n",
    "\n",
    "\n",
    "### Rubric\n",
    "Part 1 (10 points):  \n",
    "- Data Preprocessing (5 points):  \n",
    "  - Tokenization (4 points):\n",
    "    - Basic operation (0.5 point)  \n",
    "    - Handling punctuation marks and brackets (0.5 point)\n",
    "    - Handling numeric values and arithmetic operations (0.5 point)\n",
    "    - Handling contractions (2.5 point)\n",
    "  - Stopwords removal (0.5 point)\n",
    "  - Special characters removal (0.5 point)\n",
    "- Data Visualization (5 points):  \n",
    "  - Frequency-based visualization (2 points)\n",
    "  - TF-IDF based visualization (3 points)\n",
    "\n",
    "Part 2 (10 points):  \n",
    "- Data preprocessing (1 point)  \n",
    "- Training Count-based BoW Models (4.5 points)  \n",
    "  - Converting the dataset into the correct representation (1 point)  \n",
    "  - Splitting the dataset into train and test sets (0.5 point)  \n",
    "  - Training an logistic regression model (1 point)  \n",
    "  - Training an MLP model (1 point)  \n",
    "  - Reporting the correct metrics (1 point)  \n",
    "\n",
    "- Training TFIDF-based BoW Models (4.5 points)  \n",
    "  - Same rubric as the Count-base section.\n",
    "\n",
    "### Submission\n",
    "Compress this folder into a zip file named `<FIRST-NAME>_<LAST-NAME>_<BANNER-ID>.zip` and upload it to Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c5290-f3ad-46a3-9a3d-8c4fe8648214",
   "metadata": {},
   "source": [
    "# Part 1: Exploring Textual Data\n",
    "\n",
    "In this part of the assignment, you will work with a collection of text from Project Gutenberg (PG), a vast repository of free eBooks. The goal is to familiarize yourself with the process of data preparation and exploration in the context of natural language processing (NLP).  \n",
    "\n",
    "You first task is to choose at least 6 books from two genres or _Subjects_ as they are called in PG (_i.e._ at least 3 books per genre). You can browse the [Bookshelf](https://www.gutenberg.org/ebooks/bookshelf/) section to make your search easier.\n",
    "\n",
    "You will start by selecting and preprocessing a set of books from Project Gutenberg. This involves various techniques such as tokenization, punctuation removal, stopword removal. These preprocessing steps are crucial for cleaning and standardizing the text data before further analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa2de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statement of Part 1 go here.\n",
    "from scripts.part1_functions import read_and_concat_books, preprocess\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9763fca-f4b2-484d-8775-dd4c8f8bf64b",
   "metadata": {},
   "source": [
    "## Part 1.1: Loading Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f13a83",
   "metadata": {},
   "source": [
    "In this cell, invoke the `read_and_concat_books` function that takes as input a path of a folder containing a list of text files where each file contains the content of a book downloaded from the Project Gutenberg website. Under the hood the function will read each file in the directory, remove unecessary information (headers and licensing) and then will return the cleaned version.  \n",
    "\n",
    "Note that you need to perform this operation for every genre.\n",
    "\n",
    "A folder called `detective_fiction` has already been created to serve as an example for you on how `read_and_concat_books` can be invoked. Note that you are not allowed to use the same books that were provided here. You can still however use the same genere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97983b7c-3786-4f5a-92af-96c90f4734e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to use the function `read_and_concat_books`\n",
    "content = read_and_concat_books(\"./data/books/detective_fiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99d81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and concatenating books from different genres\n",
    "animals = read_and_concat_books(\"./data/books/animals\")\n",
    "detective_fiction = read_and_concat_books(\"./data/books/detective_fiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edded37-6692-4812-a3f3-74267506c175",
   "metadata": {},
   "source": [
    "## Part 1.2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc2691-2a00-4f77-86b7-c71f455fadde",
   "metadata": {},
   "source": [
    "After removing the unnecessary text, we can now start implementing the data preprocessing pipeline. This pipeline involves three key steps: tokenization, stopwords removal and special characters removal. In `scripts.part1_functions.py`, we provide the `preprocess` function that executes these steps. In order for it to properly work, you need to implement each of those operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f95db",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9d69d",
   "metadata": {},
   "source": [
    "Your task is to finish the implementation of the `tokenize` function in `scripts.part1_functions.py`. Your implementation should adhere to these tokenization rules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae034cb9",
   "metadata": {},
   "source": [
    "#### I - Basic operation\n",
    "\n",
    "The first rule of tokenization is to divide a given string into tokens by using whitespace as a separator. For instance, when applying tokenization to the string \"The quick brown fox jumped\" the resulting list of tokens would be ['The', 'quick', 'brown', 'fox', 'jumped']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b6848",
   "metadata": {},
   "source": [
    "#### II - Punctuation marks and brackets\n",
    "Your implementation should properly tokenize the following puctuation marks: `,!?;:` where each symbol should be considered as a token.  \n",
    "\n",
    "For example, the following tokenization is correct: \"What?!, the quick brown fox jumped.\" -> ['What', '?', '!', ',' 'the', 'quick', 'brown', 'fox', 'jumped', '.']  \n",
    "Whereas this tokenization would be incorrect: \"\"What?!, the quick brown fox jumped.\" -> ['What?!', 'the', 'quick', 'brown', 'fox', 'jumped.']  \n",
    "\n",
    "The same thing applies for brackets: `}, {, ), (, [, ], <, >`.  \n",
    "\n",
    "**NB**: The exception to the above rules is for the elipsis, also known as suspension points `...` This symbole should be considered as one token.  \n",
    "Example of a valid tokenization: \"The quick brown...\" -> ['The', 'quick', 'brown', '...']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf5ab83",
   "metadata": {},
   "source": [
    "#### III - Numeric values and operations\n",
    "\n",
    "In addition, tokenization should be able to properly handle numeric values such as integers, and specially floats and decimals. Moreover, it should also be able to tokenize arithmetic operations.\n",
    "For instance, `I have a GPA of 3.0.` should be tokenized as follows `['I', 'have', 'a', 'GPA', 'of', '3.0', '.']`.  \n",
    "Similarly, `3+4=7` should be split into `[3, +, 4, = , 7]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae0054",
   "metadata": {},
   "source": [
    "#### IV - Contractions\n",
    "\n",
    "Contractions in the English language are shortened forms of words or phrases created by combining two words and replacing omitted letters with an apostrophe. For example, *cannot* becomes *can't*, and *I am* becomes *I'm*. Your implementation needs to split following list of contractions:  \n",
    "- `'ll`, `'re`, `'ve`, `n't`, `'s`, `'d`, `'m`.\n",
    "\n",
    "Specifically, it should consider the above contractions as seperate tokens from the rest of the word.  \n",
    "Example of a valid tokenization: `\"I'm sure that Charlie knew that the book wasn't Bob's\"` -> `['I', \"'m\", 'sure', 'that', 'Charlie', 'knew', 'that', 'the', 'book', 'was', \"n't\", 'Bob', \"'s\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22e0a6",
   "metadata": {},
   "source": [
    "You are allowed to add additional functions to make your code more readable. <strong style=\"color: red; opacity: 0.80;\">Note that your not allowed to use external libraries to implement the tokenization task. You are however allowed to use built-in Python packages such as</strong> `string` <strong style=\"color: red; opacity: 0.80;\">and</strong> `re`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888b6c3",
   "metadata": {},
   "source": [
    "### Stopwords removal\n",
    "\n",
    "The next step of the pipeline is to remove stopwords. They are common words that appear frequently but often carry little meaning, and removing them helps to focus on the more meaningful words in the text. \n",
    "Your task is to finish the implementation of the method `remove_stopwords` in `scripts.part1_functions.py`. The list of stopwords is provided in `data/stopwords.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896089f8",
   "metadata": {},
   "source": [
    "### Special characters removal\n",
    "\n",
    "The final step is to remove special characters by finishing the implementation of the `remove_special_characters` function. This is done to standardize the text data and focus on the core content. Special characters, such as punctuation marks, symbols, and emoticons, often do not contribute to the main meaning or intent of the text. Your implementation should only keep tokens that are alphabetic (i.e. normal words), numeric (e.g. floats, integers) or alphanumeric (e.g. `CSCI4158`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36d58d",
   "metadata": {},
   "source": [
    "Once everything is implemented, invoke the `preprocess` function by passing the content of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90007a27-e0d1-4a86-b958-7a06ea3e4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to tokenize the content\n",
    "# Example: \n",
    "tokenized_content_animals = preprocess(animals)\n",
    "tokenized_content_detective_fiction = preprocess(detective_fiction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0a6c9-1f25-485d-93eb-cb9dcc7d0146",
   "metadata": {},
   "source": [
    "## Part 1.3: Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e409d08",
   "metadata": {},
   "source": [
    "In this section of the assignment, you will visualize the distributions of the tokens obtained from the preprocessing pipeline using different techniques. \n",
    "\n",
    "**NB**: You may use the results of the pipeline you have implemented, or you can reimplement it using external libaries in the cells below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29d682",
   "metadata": {},
   "source": [
    "### Frequency-based distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modified_tokenizer(text: str):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f627b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens: List, stopwords: List = []):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf197e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_remove_special_characters(tokens: List):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_preprocess(text: str):\n",
    "    functions = [modified_tokenizer, remove_stopwords, modified_remove_special_characters]\n",
    "    tokens = text\n",
    "    for function in functions:\n",
    "        tokens = function(tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e153a",
   "metadata": {},
   "source": [
    "In this task, you will write code to plot the distribution of the 10 most frequent words in the preprocessed text dataset. You have the freedom to choose between using external libraries that provide functionality for frequency counting, or implementing your own frequency counting method.  \n",
    "Make sure to label the axes and provide a clear title for the plot.  \n",
    "**NB:** You need to create a plot for each genre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining counter \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# freqency counter for animals genre\n",
    "word_freq_animals = Counter(tokenized_content_animals)\n",
    "\n",
    "# getting the 10 most frequent words\n",
    "most_common_words_animals = word_freq_animals.most_common(10)\n",
    "\n",
    "# extracting words and counts animals\n",
    "words_animals = [word for word, count in most_common_words_animals]\n",
    "counts_animals = [count for word, count in most_common_words_animals]\n",
    "\n",
    "# freqency counter for detective fiction genre\n",
    "word_freq_detective_fiction = Counter(tokenized_content_detective_fiction)\n",
    "\n",
    "# getting the 10 most frequent words\n",
    "most_common_words_detective_fiction = word_freq_detective_fiction.most_common(10)\n",
    "\n",
    "# extracting words and counts detective fiction\n",
    "words_detective_fiction = [word for word, count in most_common_words_detective_fiction]\n",
    "counts_detective_fiction = [count for word, count in most_common_words_detective_fiction]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef6b66-7fda-41a3-80ee-39f3e0e920d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for plotting the frequency distribution of words goes here\n",
    "#plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a54f61",
   "metadata": {},
   "source": [
    "### TFIDF-based distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4ebb5",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects the importance of a word in a document within a collection of documents. It is a widely used technique in text mining and information retrieval to quantify the relevance of words in a corpus.\n",
    "\n",
    "TF-IDF is calculated by multiplying two components: Term Frequency (TF) and Inverse Document Frequency (IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec4063",
   "metadata": {},
   "source": [
    "Term Frequency (TF): TF measures the frequency of a word in a specific document. It is calculated as the number of times a word appears in a document divided by the total number of words in that document. The formula for TF is: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a2869",
   "metadata": {},
   "source": [
    "$$TF(t,d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de6e41",
   "metadata": {},
   "source": [
    "where $f_{t,d}$ is the number of occurrences of term $t$ in document $d$, and the denominator is the total number of terms in document $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd0f2b",
   "metadata": {},
   "source": [
    "Inverse Document Frequency (IDF): IDF measures the importance of a word across the entire corpus. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the word. The formula for IDF is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d33a67",
   "metadata": {},
   "source": [
    "$$IDF(t) = \\log \\frac{N}{|{d \\in D : t \\in d}|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27040dd",
   "metadata": {},
   "source": [
    "where $N$ is the total number of documents in the corpus, and $|{d \\in D : t \\in d}|$ is the number of documents where term $t$ appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f0e60",
   "metadata": {},
   "source": [
    "The TF-IDF score for a word in a document is the product of its TF and IDF values:\n",
    "\n",
    "$$TF\\text{-}IDF(t,d) = TF(t,d) \\times IDF(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a738d",
   "metadata": {},
   "source": [
    "In this task, you will write code to plot the distribution of the top 10 words based on their TF-IDF scores. To calculate TF-IDF scores, you will need to slightly modify your preprocessing approach to preserve the concept of separate documents within your dataset. As previously mentioned, you can choose to use external libraries that provide TF-IDF functionality or implement your own TF-IDF calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf021a-921f-4bab-9be7-5c2bad84fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for plotting the frequency distribution of words using TF-IDF goes here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee51b8-284a-49fd-9aec-63278c153eb1",
   "metadata": {},
   "source": [
    "# Part 2: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6456e9-ab91-4c3b-ae30-7e5f081763da",
   "metadata": {},
   "source": [
    "In this part of the assignment, you will explore the application of machine learning techniques for text classification. The task at hand is to train classifiers that can distinguish between spam and legitimate (ham) SMS messages.  You will use the APIs that are provided by `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf243575-fa4e-46e7-8e99-c4c760dfbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should contain all import statements.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce3422",
   "metadata": {},
   "source": [
    "## Part 2.1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de711323",
   "metadata": {},
   "source": [
    "First, load the CSV file `data/sms/sms_data.csv` into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21131f3e-c7ee-4c22-a1a9-adfe005df52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9f004",
   "metadata": {},
   "source": [
    "Complete the implementation of the `preprocess_sms` function that will be used for tokenization. This function should:  \n",
    "- 1. Split the text into tokens.\n",
    "- 2. Lowercase each token.\n",
    "- 3. Exclude all stopwords from the list of tokens.\n",
    "- 4. Returns the list of preprocessed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99986d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation goes here.\n",
    "def preprocess_sms(text: str) -> List[str]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c399b0",
   "metadata": {},
   "source": [
    "## Part 2.2: Training a Count-based BoW Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b976099",
   "metadata": {},
   "source": [
    "In this part you will represent the data using a Bag of Words (BoW) approach. For this, you will use the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class along with the function `preprocess_sms` function as a tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf7081",
   "metadata": {},
   "source": [
    "Split your data into a training and test sets of size 80% and 20% respectively. If your method invovles randomzation (e.g. shuffling) make sure to use the value 42 as a seed for reproducibility.  \n",
    "\n",
    "Use the `train_test_split` function provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your to split the data into training and testing sets goes here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119d5e8",
   "metadata": {},
   "source": [
    "Now, you should have your data ready to start training and evaluating a set of classifiers. Specifically, you will train a Linear Regression model and a simple one-layer MLP (Multilayer Perceptron) model. For the MLP model, you will need to run hyperparameter finetuning on at least two hyperparameters (e.g. learning rate, number of neurons in the hidden layer)\n",
    "\n",
    "To evaluate the above classifiers, you need to report the following metrics:\n",
    "- 1. F1 score\n",
    "- 2. Precision\n",
    "- 3. Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4634db",
   "metadata": {},
   "source": [
    "You will need to use the following APIs:  \n",
    "- [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  \n",
    "- [`MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "- The [`sklearn.metrics`](https://scikit-learn.org/1.5/api/sklearn.metrics.html) package that contain implementations of different classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef732ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to train a logistic regression model and evaluate its performance goes here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to train an MLP model and evaluate its performance goes here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58f56b",
   "metadata": {},
   "source": [
    "## Part 2.3: Training a TDIDF-based BoW Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39d435",
   "metadata": {},
   "source": [
    "In this part, you will replicate the the steps of Part 1.2 with the exception of replacing the wordcount as a feature representation with TFIDF count. For this task, you will need to use the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class. The remainder of the classes is the same as the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279778b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149aa0e-e059-497f-aaf0-cdc155acb70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
